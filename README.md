# Deep Learning
## 介绍
- 神经网络：通过模仿人脑神经元工作的原理，可以处理并分析复杂的信息，从而获取智能的学习能力。在各领域中应用广泛。
## 技术用途
- 可广泛应用于图像识别、语音识别、医药、金融、广告等领域。
## 方法实现
### 卷积层
- 每个卷积层储存一个卷积核矩阵，对输入中每一个卷积核矩阵大小的范围与卷积核作元素乘法，并相加，即为卷积核对输入作卷积操作。输入中每一个卷积核矩阵大小
称为感受野，感受野在输入的高和宽的方向（以输入是图片为例），每相隔slide个元素取出。
- 使用for循环实现的版本由于涉及到4个维度（以conv2d为例），运行速度太慢。因此使用img2col的技巧，将输入的每一个感受野拼接成一个新的矩阵，卷积核也
reshape成一列或一行，然后对两个矩阵作矩阵乘法，最后将结果reshape回原size即可。
### 池化层
- MaxPool：将kernel$(k\times k)$矩阵范围变为1个元素，在该范围内取最大值。
- AveragePool：将kernel$(k\times k)$矩阵范围变为1个元素，在该范围内取均值。
### BN层
- 一般放置在激活函数的前面，其作用是将输入归一化成0均值，方差为1的数据，使得数据在进入激活函数之前不会过于分散或者过于聚集，使得反向传播时梯度放大
其分散或聚集的效果，即预防了梯度消失或者梯度爆炸的情况发生。
### 全连接层
- 每个全连接层储存一个大小为(in, out)的权重矩阵，in为输入神经元数量，out为输出神经元数量。对权重的转置和输入作矩阵乘法，得到输出。
### 激活函数
- Sigmoid：将输入映射到0和1之间的值。若输入是很大或很小的值的时候，反向传播时其梯度会无限接近于0，导致梯度传播时可能出现梯度消失的情况，
即参数不更新，或者说更新得很慢。
- Relu：忽略负值的输入，只接受正值的输入。解决了梯度消失的问题，但有可能引起梯度爆炸的情况，因为其导数要么为0，要么为1，若反向传播时上一层的梯度很大，
则传播到该层的梯度同样很大，导致参数更新过快。若其导数为0，则可能导致某些参数永远得不到更新。
- Tanh：将输入映射到-1到1之间。其收敛速度比Sigmoid稍快，但是仍然存在梯度消失的情况。
### 损失函数
- MSE(Mean Square Error)：常用于线性回归，即函数拟合。
- CE(Cross Entropy)：通常和softmax一起绑定使用。常用于逻辑回归，即分类。
## 实验结果展示
### LeNet
在训练过程中的准确率变化以及损失：  
![LeNet](https://github.com/FFFjx/Deep-Learning/blob/main/pic/LeNet-train.png)  
训练集的损失随着迭代轮次的增加而逐渐降低，而训练集的准确率则随着迭代轮次的增加而增加，直至收敛。证明网络学习有效。  
在测试过程中的准确率变化：  
![LeNet](https://github.com/FFFjx/Deep-Learning/blob/main/pic/LeNet-test.png)  
经过20个epoch迭代，其在测试集的准确率已经可以超过96%。
